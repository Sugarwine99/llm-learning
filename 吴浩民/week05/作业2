一、 技术架构概述
本方案采用 双塔模型（Bi-Encoder） 架构。其核心思想是将 FAQ 库中的问题和用户的提问分别通过 BERT 转换成高维向量，通过计算向量空间中的距离来衡量语义相似度。

二、 核心技术流程
1. 文本编码阶段 (Text Encoding)
输入处理：将原始文本转换为 BERT 要求的输入格式（Input IDs, Attention Mask, Token Type IDs）。

模型表征：文本进入预训练的 BERT 模型（如 bert-base-chinese），经过 12 层 Transformer Encoder 提取特征。

向量池化 (Pooling)：BERT 的输出是每个 Token 的向量，需通过池化策略得到句向量。
2. 相似度计算阶段 (Similarity Calculation)
向量归一化：为了消除文本长度对分值的影响，对生成的向量进行 L_2 归一化。

余弦相似度 (Cosine Similarity)：计算用户问题向量 u 与 FAQ 库向量 v 之间的夹角余弦值。

分值映射：将结果映射到 [0, 1] 区间，分值越接近 1，表示语义越相似。

三、 工程化实施方案
离线索引 (Offline)：后端定时将 FAQ 库中的“标准问”通过算法模型转为 768 维向量，存入 Milvus 或 Faiss 向量数据库中，并构建 HNSW 索引 以加速检索。

在线检索 (Online)：

用户提问后，算法 API 实时将其编码为向量。

调用向量数据库进行 Top-K 近似最近邻搜索 (ANN)。

设定阈值（如 0.85），高于阈值的最高分结果即为命中，返回对应答案。
